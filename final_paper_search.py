#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¤šå¹³å°å­¦æœ¯è®ºæ–‡æ£€ç´¢è„šæœ¬
é€šè¿‡HTTPæ¥å£è°ƒç”¨MCPæœåŠ¡æ¥æ£€ç´¢å¤šä¸ªå¹³å°çš„å­¦æœ¯è®ºæ–‡ï¼ŒæŒ‰æ—¶é—´æ’åºï¼Œå»é‡ï¼Œè¾“å‡ºJSONæ ¼å¼

ä½¿ç”¨æ–¹æ³•:
1. å…ˆå¯åŠ¨MCP HTTPæœåŠ¡: python http_mcp_server.py
2. è¿è¡Œæ­¤è„šæœ¬: python final_paper_search.py "æœç´¢å…³é”®è¯" [è®ºæ–‡æ•°é‡]

ç¤ºä¾‹:
python final_paper_search.py "quantum computing" 50
python final_paper_search.py "machine learning" 30
"""

import requests
import json
import sys
import time
import hashlib
from datetime import datetime
from typing import List, Dict, Set

# è®¾ç½®è¾“å‡ºç¼–ç 
if sys.platform == "win32":
    import codecs
    sys.stdout = codecs.getwriter("utf-8")(sys.stdout.detach())


class PaperSearchClient:
    """å¤šå¹³å°å­¦æœ¯è®ºæ–‡æ£€ç´¢å®¢æˆ·ç«¯"""
    
    def __init__(self, base_url: str = "http://localhost:8011"):
        self.base_url = base_url
        self.session = requests.Session()
        self.platforms = ['arxiv', 'pubmed', 'biorxiv', 'medrxiv', 'google_scholar', 'iacr', 'semantic']
        self.seen_papers: Set[str] = set()  # ç”¨äºå»é‡
    
    def check_server_status(self) -> bool:
        """æ£€æŸ¥MCPæœåŠ¡å™¨çŠ¶æ€"""
        try:
            response = self.session.get(f"{self.base_url}/", timeout=5)
            return response.status_code == 200
        except:
            return False
    
    def generate_paper_hash(self, title: str, authors: List[str]) -> str:
        """ç”Ÿæˆè®ºæ–‡çš„å”¯ä¸€æ ‡è¯†ç¬¦ç”¨äºå»é‡"""
        # æ ‡å‡†åŒ–æ ‡é¢˜ï¼ˆå»é™¤ç©ºæ ¼ã€æ ‡ç‚¹ï¼Œè½¬å°å†™ï¼‰
        normalized_title = ''.join(c.lower() for c in title if c.isalnum())
        # æ ‡å‡†åŒ–ä½œè€…åˆ—è¡¨
        normalized_authors = ''.join(sorted([
            ''.join(c.lower() for c in author if c.isalnum()) 
            for author in authors[:2]  # åªä½¿ç”¨å‰ä¸¤ä¸ªä½œè€…
        ]))
        # ç”Ÿæˆå“ˆå¸Œ
        content = f"{normalized_title}_{normalized_authors}"
        return hashlib.md5(content.encode()).hexdigest()
    
    def search_platform(self, platform: str, query: str, max_results: int = 10) -> List[Dict]:
        """åœ¨å•ä¸ªå¹³å°æœç´¢è®ºæ–‡"""
        try:
            payload = {
                "query": query,
                "max_results": max_results
            }
            
            print(f"æ­£åœ¨æœç´¢ {platform}...")
            response = self.session.post(
                f"{self.base_url}/search/{platform}",
                json=payload,
                timeout=60
            )
            
            if response.status_code == 200:
                data = response.json()
                papers = data.get('papers', [])
                print(f"{platform} æ‰¾åˆ° {len(papers)} ç¯‡è®ºæ–‡")
                return papers
            else:
                print(f"æœç´¢ {platform} å¤±è´¥: {response.status_code}")
                return []
        
        except Exception as e:
            print(f"æœç´¢ {platform} æ—¶å‡ºé”™: {e}")
            return []
    
    def search_all_platforms(self, query: str, total_papers: int = 50) -> List[Dict]:
        """æœç´¢æ‰€æœ‰å¹³å°å¹¶åˆå¹¶ç»“æœ"""
        all_papers = []
        
        # è®¡ç®—æ¯ä¸ªå¹³å°åº”è¯¥æœç´¢çš„è®ºæ–‡æ•°é‡
        papers_per_platform = max(10, total_papers // len(self.platforms))
        
        for platform in self.platforms:
            papers = self.search_platform(platform, query, papers_per_platform)
            
            for paper in papers:
                try:
                    # æ ‡å‡†åŒ–è®ºæ–‡æ ¼å¼
                    standardized = self.standardize_paper(paper, platform)
                    
                    # æ£€æŸ¥æ˜¯å¦é‡å¤
                    paper_hash = self.generate_paper_hash(
                        standardized['title'], 
                        standardized['authors']
                    )
                    
                    if paper_hash not in self.seen_papers and standardized['title']:
                        self.seen_papers.add(paper_hash)
                        all_papers.append(standardized)
                    else:
                        print(f"è·³è¿‡é‡å¤è®ºæ–‡: {standardized['title'][:50]}...")
                
                except Exception as e:
                    print(f"å¤„ç†è®ºæ–‡æ—¶å‡ºé”™: {e}")
                    continue
        
        # æŒ‰å¹´ä»½æ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
        all_papers.sort(key=lambda x: x.get('year', '0'), reverse=True)
        
        return all_papers[:total_papers]
    
    def standardize_paper(self, paper: Dict, source: str) -> Dict:
        """æ ‡å‡†åŒ–è®ºæ–‡æ ¼å¼ï¼Œç¡®ä¿ç¬¦åˆè¦æ±‚çš„JSONæ ¼å¼"""
        # å¤„ç†ä½œè€…
        authors_raw = paper.get('authors', '')
        if isinstance(authors_raw, str):
            if ';' in authors_raw:
                authors = [a.strip() for a in authors_raw.split(';') if a.strip()]
            elif ',' in authors_raw and len(authors_raw.split(',')) > 2:
                authors = [a.strip() for a in authors_raw.split(',') if a.strip()]
            else:
                authors = [authors_raw] if authors_raw else []
        elif isinstance(authors_raw, list):
            authors = authors_raw
        else:
            authors = []
        
        # æå–å¹´ä»½
        published_date = paper.get('published_date', '')
        year = ''
        if published_date:
            try:
                if 'T' in published_date:
                    year = published_date.split('T')[0].split('-')[0]
                elif '-' in published_date:
                    year = published_date.split('-')[0]
                else:
                    year = published_date[:4] if len(published_date) >= 4 else ''
                
                # éªŒè¯å¹´ä»½
                if year and (not year.isdigit() or int(year) < 1900 or int(year) > 2030):
                    year = ''
            except:
                year = ''
        
        # ç¡®ä¿æ‰€æœ‰å¿…éœ€å­—æ®µéƒ½å­˜åœ¨
        title = paper.get('title') or ''
        abstract = paper.get('abstract') or ''
        url = paper.get('url') or paper.get('pdf_url') or ''
        venue = paper.get('source') or source
        paper_id = paper.get('paper_id') or ''

        return {
            "title": title.strip() if title else '',
            "authors": authors,
            "abstract": abstract.strip() if abstract else '',
            "year": year,
            "url": url,
            "venue": venue,
            "citedby": paper.get('citations', 0),
            "paper_id": paper_id
        }


def main():
    """ä¸»å‡½æ•°"""
    if len(sys.argv) < 2:
        print("ç”¨æ³•: python final_paper_search.py <æœç´¢å…³é”®è¯> [è®ºæ–‡æ•°é‡]")
        print("ç¤ºä¾‹: python final_paper_search.py 'quantum computing' 50")
        print("ç¤ºä¾‹: python final_paper_search.py 'machine learning' 30")
        print()
        print("æ³¨æ„: è¯·å…ˆå¯åŠ¨MCP HTTPæœåŠ¡: python http_mcp_server.py")
        sys.exit(1)
    
    query = sys.argv[1]
    total_papers = int(sys.argv[2]) if len(sys.argv) > 2 else 50
    
    print("=" * 60)
    print("å¤šå¹³å°å­¦æœ¯è®ºæ–‡æ£€ç´¢ç³»ç»Ÿ")
    print("=" * 60)
    print(f"æœç´¢ä¸»é¢˜: {query}")
    print(f"ç›®æ ‡è®ºæ–‡æ•°é‡: {total_papers}")
    print(f"æ”¯æŒå¹³å°: arXiv, PubMed, bioRxiv, medRxiv, Google Scholar, IACR, Semantic Scholar")
    print("-" * 60)
    
    # åˆ›å»ºå®¢æˆ·ç«¯
    client = PaperSearchClient()
    
    # æ£€æŸ¥æœåŠ¡å™¨çŠ¶æ€
    print("æ£€æŸ¥MCPæœåŠ¡å™¨çŠ¶æ€...")
    if not client.check_server_status():
        print("âŒ é”™è¯¯: æ— æ³•è¿æ¥åˆ°MCPæœåŠ¡å™¨")
        print("è¯·å…ˆå¯åŠ¨HTTP MCPæœåŠ¡å™¨: python http_mcp_server.py")
        sys.exit(1)
    
    print("âœ… MCPæœåŠ¡å™¨è¿æ¥æ­£å¸¸")
    print("-" * 60)
    
    # æ‰§è¡Œæœç´¢
    start_time = time.time()
    papers = client.search_all_platforms(query, total_papers)
    end_time = time.time()
    
    print("-" * 60)
    print(f"ğŸ‰ æœç´¢å®Œæˆ! æ‰¾åˆ° {len(papers)} ç¯‡ä¸é‡å¤çš„è®ºæ–‡")
    print(f"â±ï¸  ç”¨æ—¶: {end_time - start_time:.2f} ç§’")
    
    if not papers:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°ç›¸å…³è®ºæ–‡ï¼Œè¯·å°è¯•å…¶ä»–å…³é”®è¯")
        sys.exit(1)
    
    # ä¿å­˜ç»“æœåˆ°JSONæ–‡ä»¶
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    output_file = f"papers_{query.replace(' ', '_')}_{timestamp}.json"
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(papers, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    
    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    years = [p['year'] for p in papers if p['year']]
    if years:
        print(f"ğŸ“Š å¹´ä»½åˆ†å¸ƒ: {min(years)} - {max(years)}")
    
    venues = {}
    for paper in papers:
        venue = paper.get('venue', 'Unknown')
        venues[venue] = venues.get(venue, 0) + 1
    
    print("ğŸ“ˆ æ¥æºåˆ†å¸ƒ:")
    for venue, count in sorted(venues.items(), key=lambda x: x[1], reverse=True):
        print(f"   {venue}: {count} ç¯‡")
    
    # æ˜¾ç¤ºå‰å‡ ç¯‡è®ºæ–‡çš„é¢„è§ˆ
    print("\nğŸ“„ å‰5ç¯‡è®ºæ–‡é¢„è§ˆ:")
    print("=" * 60)
    for i, paper in enumerate(papers[:5], 1):
        print(f"\n{i}. {paper['title']}")
        authors_str = ', '.join(paper['authors'][:3]) if paper['authors'] else 'Unknown'
        if len(paper['authors']) > 3:
            authors_str += f' ç­‰ {len(paper["authors"])} ä½ä½œè€…'
        print(f"   ğŸ‘¥ ä½œè€…: {authors_str}")
        print(f"   ğŸ“… å¹´ä»½: {paper['year'] or 'Unknown'}")
        print(f"   ğŸ›ï¸  æ¥æº: {paper['venue']}")
        print(f"   ğŸ“Š å¼•ç”¨: {paper['citedby']}")
        abstract_preview = paper['abstract'][:200] + "..." if len(paper['abstract']) > 200 else paper['abstract']
        print(f"   ğŸ“ æ‘˜è¦: {abstract_preview}")
    
    print(f"\nğŸ“ å®Œæ•´ç»“æœè¯·æŸ¥çœ‹æ–‡ä»¶: {output_file}")
    print("=" * 60)


if __name__ == "__main__":
    main()
